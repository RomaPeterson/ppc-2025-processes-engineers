# Быстрая сортировка с четно-нечетным слиянием Бэтчера

- Студент: Петерсон Роман Дмитриевич, группа 3823Б1ПР5  
- Технология: MPI, SEQ  
- Вариант: 15  

## 1. Введение
Сортировка больших массивов данных является одной из фундаментальных задач в вычислительной технике. Для ускорения обработки больших объемов данных применяются параллельные алгоритмы. Данный проект реализует гибридный алгоритм, сочетающий локальную быструю сортировку (QuickSort) на каждом MPI-процессе с последующим четно-нечетным слиянием Бэтчера для глобальной сортировки распределенных данных.

## 2. Постановка задачи
Отсортировать массив вещественных чисел в порядке возрастания, используя распределенную архитектуру.

**Входные данные:** Вектор `double` произвольной длины (возможно, пустой).  
**Выходные данные:** Вектор `double` той же длины, отсортированный в порядке возрастания.  
**Требования:** Алгоритм должен корректно работать для любых входных данных, включая дубликаты, отрицательные числа, уже отсортированные или обратно отсортированные массивы.

## 3. Базовый алгоритм (последовательный)
**Алгоритм в `ops_seq.cpp`:**
1. Проверить, что входной массив не пуст.
2. Использовать стандартную сортировку `std::ranges::sort`.
3. Вернуть отсортированный массив.

**Сложность:** O(n log n).  
**Особенности:** Используется встроенная оптимизированная сортировка стандартной библиотеки.

## 4. Схема распараллеливания (MPI)
**Гибридная архитектура:**
1. **Распределение данных:** 
   - Массив дополняется до длины, кратной числу процессов (padding +∞)
   - Данные равномерно распределяются по процессам с помощью `MPI_Scatterv`

2. **Локальная сортировка:**
   - Каждый процесс сортирует свой блок с помощью `std::ranges::sort`

3. **Глобальная сортировка (сеть Бэтчера):**
   - Применяется алгоритм четно-нечетного слияния Бэтчера
   - Строится сеть компараторов для попарного сравнения блоков
   - На каждом шаге пары процессов обмениваются данными и сливают их

4. **Сбор результатов:**
   - Собираются отсортированные блоки с помощью `MPI_Gatherv`
   - Удаляются элементы padding
   - Результат рассылается всем процессам

**Коммуникационная схема:**
- `MPI_Scatterv` для начального распределения
- `MPI_Sendrecv` для попарного обмена при слиянии
- `MPI_Gatherv` для сбора результатов
- `MPI_Bcast` для рассылки финального результата

## 5. Детали реализации
**Структура кода:**
- `ops_seq.hpp/cpp` – последовательная реализация
- `ops_mpi.hpp/cpp` – MPI-параллельная реализация с сетью Бэтчера
- `common.hpp` – общие определения типов
- `main.cpp` – функциональные и производительные тесты

**Ключевые компоненты MPI-версии:**
- `TransmitDimensions()` – вычисление размеров с padding
- `DispatchData()` – распределение данных между процессами
- `CreateComparatorSet()` – построение сети компараторов Бэтчера
- `ExecuteComparisons()` – выполнение попарных слияний
- `CombineSegments()` – слияние двух отсортированных блоков

**Особенности реализации:**
- Используется стековая реализация алгоритма Бэтчера вместо рекурсии
- Padding выполняется значением `+∞` для сохранения порядка
- Направление слияния определяется по первому элементу пары компараторов
- Поддерживается произвольное число процессов (не только степени двойки)

## 6. Экспериментальная установка
**Аппаратное обеспечение:**
- Процессор: Intel Core i5-9400F (6 ядер, 6 потоков) @ 2.90 ГГц
- ОЗУ: 16 ГБ DDR4
- ОС: Ubuntu 20.04.4 LTS

**Программное обеспечение:**
- Компилятор: GCC 9.4.0 с флагами `-O3`
- MPI: OpenMPI 4.0.3
- Тестирование: GoogleTest 1.11.0

**Параметры тестирования:**
- Размер данных: 8 миллионов элементов (1000000 × 8)
- Тип данных: `double` в диапазоне [-1000.0, 1000.0]
- Число процессов: 1–6
- Seed для генерации: 42

## 7. Результаты и обсуждение

### 7.1 Корректность
Проверка корректности выполнена с помощью 12 тестовых случаев:
- Пустой массив и массив из одного элемента
- Массивы с дубликатами и отрицательными числами
- Уже отсортированные и обратно отсортированные массивы
- Массивы с четным/нечетным количеством элементов
- Массивы с широким диапазоном значений и высокой точностью

Все тесты проходят успешно, результаты MPI и SEQ версий совпадают.

### 7.2 Производительность
Результаты для массива из 8 миллионов элементов:

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| SEQ   | 1        | 3.45     | 1.00      | –             |
| MPI   | 2        | 1.92     | 1.80      | 90.0%         |
| MPI   | 4        | 1.08     | 3.19      | 79.8%         |
| MPI   | 6        | 0.81     | 4.26      | 71.0%         |

**Анализ результатов:**
1. **Хорошее ускорение:** На 6 процессах достигнуто ускорение 4.26×
2. **Высокая начальная эффективность:** На 2 процессах эффективность составляет 90%
3. **Снижение эффективности:** С ростом числа процессов эффективность падает до 71% на 6 процессах
4. **Основные накладные расходы:**
   - Коммуникация между процессами при слиянии
   - Дополнение массива (padding) до кратности числу процессов
   - Построение и выполнение сети компараторов Бэтчера

**Сравнение с идеальным ускорением:**
- Линейное ускорение ограничено коммуникационными затратами
- Алгоритм Бэтчера имеет сложность O(log² p) шагов слияния
- Для 6 процессов требуется 3 этапа сравнений

## 8. Выводы
1. **Реализован корректный алгоритм:** Гибридная сортировка с QuickSort и четно-нечетным слиянием Бэтчера работает корректно для всех тестовых случаев.
2. **Достигнуто хорошее ускорение:** На 6 процессах ускорение составляет 4.26× при эффективности 71%.
3. **Алгоритм масштабируется:** С ростом размера данных эффективность параллелизации улучшается.
4. **Гибкость реализации:** Поддерживается произвольное число процессов и размер данных.

**Преимущества:**
- Хорошая балансировка нагрузки при равномерном распределении данных
- Минимальные требования к памяти на каждом процессе
- Устойчивость к различным распределениям входных данных

**Ограничения:**
- Требуется дополнение массива при неравномерном распределении
- Сеть Бэтчера оптимальна для степеней двойки процессов
- Коммуникационные затраты растут с увеличением числа процессов

## 9. Литература

1. Документация по курсу: "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А, Нестеров А.Ю)
2. Лекции по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/