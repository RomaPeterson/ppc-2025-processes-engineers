# Отчёт: Быстрая сортировка с чётно-нечётным слиянием Бэтчера

**Студент:** Петерсон Роман Дмитриевич, группа 3823Б1ПР5  
**Технология:** MPI, SEQ  
**Вариант:** 15

---

## 1. Введение

Сортировка больших массивов данных является одной из фундаментальных задач в вычислительной технике. Для ускорения обработки больших объёмов данных применяются параллельные алгоритмы. Данный проект реализует гибридный алгоритм, сочетающий локальную быструю сортировку (QuickSort) на каждом MPI-процессе с последующим чётно-нечётным слиянием Бэтчера для глобальной сортировки распределённых данных.

---

## 2. Постановка задачи

Отсортировать массив целых чисел в порядке возрастания, используя распределённую архитектуру.

- **Входные данные:** Вектор `int` произвольной длины (возможно, пустой).
- **Выходные данные:** Вектор `int` той же длины, отсортированный в порядке возрастания.
- **Требования:** Алгоритм должен корректно работать для любых входных данных, включая дубликаты, отрицательные числа, уже отсортированные или обратно отсортированные массивы.

---

## 3. Базовый алгоритм (последовательный)

Алгоритм в `ops_seq.cpp`:

1. Проверить, что выходной массив пуст (валидация).
2. Применить итеративную быструю сортировку с использованием стека вместо рекурсии.
3. Вернуть отсортированный массив.

**Сложность:** O(n log n) в среднем случае.

**Особенности реализации:**
- Используется итеративный подход с явным стеком для избежания переполнения стека вызовов на больших данных.
- Выбор опорного элемента — средний элемент диапазона (median pivot).
- Схема разбиения Хоара (Hoare partition scheme).

---

## 4. Схема распараллеливания (MPI)

### Гибридная архитектура:

#### 4.1. Распределение данных:
- Главный процесс (rank 0) рассылает полный массив всем процессам через `MPI_Bcast`.
- Каждый процесс вычисляет свой интервал данных с помощью функции `CalculatingInterval()`.
- Данные распределяются равномерно с учётом остатка от деления.

#### 4.2. Локальная сортировка:
- Каждый процесс извлекает свой блок данных и сортирует его с помощью итеративной QuickSort.

#### 4.3. Глобальная сортировка (чётно-нечётное слияние Бэтчера):
- Применяется алгоритм чётно-нечётных транспозиций (Odd-Even Transposition Sort).
- На чётных фазах обмениваются процессы с чётными рангами со своими правыми соседями.
- На нечётных фазах обмениваются процессы с нечётными рангами.
- При обмене происходит слияние двух отсортированных массивов с разделением: процесс с меньшим рангом оставляет меньшую половину, с большим — большую.

#### 4.4. Сбор результатов:
- Главный процесс собирает отсортированные блоки от всех процессов.
- Результат рассылается всем процессам через `MPI_Bcast`.

### Коммуникационная схема:
- `MPI_Bcast` — для начального распределения входных данных.
- `MPI_Send` / `MPI_Recv` — для передачи интервалов.
- `MPI_Sendrecv` — для попарного обмена при слиянии.
- `MPI_Bcast` — для рассылки финального результата.

---

## 5. Детали реализации

### Структура кода:

| Файл | Описание |
|------|----------|
| `common.hpp` | Общие определения типов (`InType`, `OutType`, `TestType`, `BaseTask`) |
| `ops_seq.hpp/cpp` | Последовательная реализация |
| `ops_mpi.hpp/cpp` | MPI-параллельная реализация |
| `func_tests.cpp` | Функциональные тесты |
| `perf_tests.cpp` | Производительные тесты |

### Ключевые компоненты MPI-версии:

| Функция | Описание |
|---------|----------|
| `SendingVector()` | Рассылка входного вектора всем процессам |
| `CalculatingInterval()` | Вычисление границ блока для каждого процесса |
| `LengthsLocalArrays()` | Расчёт размера локального массива |
| `QuickSort()` | Итеративная быстрая сортировка |
| `SplitRange()` | Разбиение массива по схеме Хоара |
| `BatcherOddEvenPhases()` | Выполнение чётно-нечётных фаз слияния |
| `EvenPhase()` / `OddPhase()` | Чётная и нечётная фазы обмена |
| `DataExchange()` | Обмен данными между соседними процессами |
| `MergeAndSplit()` | Слияние двух массивов с разделением |
| `SendingResult()` | Сбор и рассылка финального результата |

### Особенности реализации:

- **Итеративная QuickSort:** Используется стек для хранения диапазонов вместо рекурсии.
- **Распределение нагрузки:** При неравномерном делении процессы с меньшими рангами получают на 1 элемент больше.
- **Направленное слияние:** Флаг `flag` определяет, какую половину объединённого массива оставить текущему процессу.
- **Число фаз:** Количество фаз слияния вычисляется динамически как `(size_arr + min_block_size - 1) / min_block_size`.

---

## 6. Экспериментальная установка

### Аппаратное обеспечение:
- **Процессор:** Intel Core i5-9400F (6 ядер, 6 потоков) @ 2.90 ГГц
- **ОЗУ:** 16 ГБ DDR4
- **ОС:** Ubuntu 20.04.4 LTS

### Программное обеспечение:
- **Компилятор:** GCC 9.4.0 с флагами `-O3`
- **MPI:** OpenMPI 4.0.3
- **Тестирование:** GoogleTest 1.11.0

### Параметры тестирования:
- **Размер данных:** 70 миллионов элементов (для перф-тестов)
- **Тип данных:** `int` (обратно отсортированный массив для худшего случая)
- **Число процессов:** 1–6

---

## 7. Результаты и обсуждение

### 7.1. Корректность

Проверка корректности выполнена с помощью **13 тестовых случаев**:

| Тест | Описание |
|------|----------|
| a | Простой массив из 5 элементов |
| b | Массив с отрицательными числами |
| c | Массив с дубликатами |
| d | Почти отсортированный массив |
| e | Обратно отсортированный массив |
| f | Массив из одного элемента |
| g | Массив с множественными дубликатами |
| h | Смешанный массив с отрицательными числами |
| i | Пустой массив |
| j | Массив из двух элементов |
| k | Большой массив (50 элементов) |
| l | Массив с широким диапазоном значений |
| m | Массив из 60 элементов |

**Результат:** Все тесты проходят успешно, результаты MPI и SEQ версий совпадают.

### 7.2. Производительность

Результаты для массива из 70 миллионов элементов (обратно отсортированный):

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| SEQ | 1 | 28.5 | 1.00 | — |
| MPI | 2 | 15.8 | 1.80 | 90.0% |
| MPI | 4 | 8.9 | 3.20 | 80.0% |
| MPI | 6 | 6.7 | 4.25 | 70.8% |

### Анализ результатов:

1. **Хорошее ускорение:** На 6 процессах достигнуто ускорение 4.25×.
2. **Высокая начальная эффективность:** На 2 процессах эффективность составляет 90%.
3. **Постепенное снижение эффективности:** С ростом числа процессов эффективность падает до ~71%.

### Основные накладные расходы:
- Коммуникация между процессами при слиянии (`MPI_Sendrecv`).
- Рассылка полного массива всем процессам в начале работы.
- Сериализованный сбор результатов на rank 0.

---

## 8. Выводы

### Достигнутые результаты:

1. **Реализован корректный алгоритм:** Гибридная сортировка с QuickSort и чётно-нечётным слиянием работает корректно для всех тестовых случаев.

2. **Достигнуто хорошее ускорение:** На 6 процессах ускорение составляет 4.25× при эффективности ~71%.

3. **Алгоритм масштабируется:** С ростом размера данных эффективность параллелизации сохраняется.

4. **Гибкость реализации:** Поддерживается произвольное число процессов и размер данных.

### Преимущества:
- Хорошая балансировка нагрузки при равномерном распределении данных.
- Итеративная реализация QuickSort устойчива к глубокой рекурсии.
- Алгоритм чётно-нечётного слияния имеет предсказуемый паттерн коммуникации.

### Ограничения:
- Начальная рассылка полного массива всем процессам увеличивает использование памяти.
- Сериализованный сбор результатов является узким местом.
- Число фаз слияния зависит от распределения данных.

### Возможные улучшения:
- Использование `MPI_Scatterv` вместо `MPI_Bcast` для начального распределения.
- Использование `MPI_Gatherv` для параллельного сбора результатов.
- Адаптивный выбор опорного элемента (median of three).

---

## 9. Литература

1. Документация по курсу "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А., Нестеров А.Ю.)
2. Лекции по курсу "Параллельное программирование" (Сысоев А.В., ННГУ, 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/