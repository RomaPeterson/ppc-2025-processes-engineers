# Поиск минимального элемента в каждом столбце матрицы

- Студент: Петерсон Роман Дмитриевич, группа 3823Б1ПР5
- Технология: MPI, SEQ
- Вариант: 18

## 1. Введение
Задача поиска минимальных элементов по столбцам матрицы является фундаментальной в обработке данных, анализе изображений и предобработке данных для машинного обучения. Для больших матриц (например, 10⁴×10⁴) последовательные вычисления становятся узким местом. Данный проект реализует и оценивает параллельное решение на основе MPI для ускорения поиска минимумов по столбцам.

## 2. Постановка задачи
Дана целочисленная матрица A размером m × n, требуется вычислить вектор b длины n, где:
b[j] = min{A[i][j] | i = 0..m-1} для всех j = 0..n-1.

**Формат входных данных:** Кортеж (m, n, values), где values — плоский массив построчного хранения.
**Формат выходных данных:** Вектор из n целых чисел (минимумы столбцов).
**Ограничения:** m, n > 0; для MPI: n ≤ INT_MAX (из-за параметра MPI_Reduce).

## 3. Базовый алгоритм (последовательный)
1. Инициализировать результирующий вектор размером n значением INT_MAX.
2. Для каждой строки i = 0 до m-1:
   - Для каждого столбца j = 0 до n-1:
     - result[j] = min(result[j], A[i][j])
3. Вернуть результирующий вектор.
**Сложность:** O(m × n) времени, O(n) дополнительной памяти.

## 4. Схема распараллеливания (MPI)
**Распределение данных:** Строки делятся равномерно между процессами. Если m % size ≠ 0, первые (m % size) процессов получают на одну строку больше.

**Коммуникационная схема:**
1. Каждый процесс вычисляет локальные минимумы для своих строк.
2. MPI_Reduce с операцией MPI_MIN собирает глобальные минимумы на процессе 0.
3. MPI_Bcast рассылает конечный результат всем процессам.

**Роли процессов:** Все процессы вычисляют; процесс 0 выступает корневым для редукции и рассылки.

**Псевдокод:**
```
local_min[0:n-1] = INT_MAX
для каждой локальной строки i:
    для j = 0 до n-1:
        local_min[j] = min(local_min[j], A[i][j])
MPI_Reduce(local_min → global_min, MPI_MIN, root=0)
MPI_Bcast(global_min от root=0)
```

## 5. Детали реализации
**Структура кода:**
- `ops_seq.hpp/cpp` – Последовательная реализация
- `ops_mpi.hpp/cpp` – MPI-параллельная реализация
- `common.hpp` – Общие определения типов (InType, OutType)
- `main.cpp` – Функциональные и производительные тесты

**Ключевые классы:**
- `PetersonRMinMatrixColsElmSEQ` – Последовательная задача
- `PetersonRMinMatrixColsElmMPI` – MPI-параллельная задача
Оба наследуются от `BaseTask` (ppc::task::Task).

**Предположения и граничные случаи:**
- Проверка входных данных включает положительные размеры и соответствие количества элементов.
- MPI-версия проверяет, что n ≤ INT_MAX.
- Пустые матрицы (m=0 или n=0) считаются невалидными.

**Использование памяти:** Каждый MPI-процесс хранит свои локальные строки (≈ m/size × n элементов) и вектор локальных минимумов (n элементов).

## 6. Экспериментальная установка
**Аппаратное обеспечение/ОС:**
- Процессор: Intel Core i5-9400F (6 ядер, 6 потоков) @ 2.90 ГГц
- ОЗУ: 16 ГБ DDR4
- ОС: Ubuntu 20.04.4 LTS

**Инструментарий:**
- Компилятор: GCC 9.4.0
- Тип сборки: RelWithDebInfo (оптимизация -O3)
- MPI: OpenMPI 4.0.3
- Тестирование: GoogleTest 1.11.0

**Окружение:**
- MPI-процессы: 1–6 (задаются через `mpirun -np N`)
- Пути к тестовым данным: `test_matrix_3_3.txt`, `test_matrix_4_5_neg.txt`

**Генерация данных:** В производительном тесте используется сгенерированная матрица 10000×10000 с равномерным распределением [-10, 20] (seed=123).

## 7. Результаты и обсуждение

### 7.1 Корректность
Проверено с помощью:
- Юнит-тестов на невалидные входные данные (нулевые размеры, несоответствие размеров).
- Функциональных тестов с известными матрицами (3×3, 4×5).
- Перекрестной проверки выходных данных SEQ и MPI на 6 тестовых случаях.
- Сравнения с независимо вычисленными минимумами.

### 7.2 Производительность
Результаты для матрицы 10000×10000:

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|----------------------|----------|-----------|---------------|
| seq   | 1                    | 1.85     | 1.00      | –             |
| mpi   | 2                    | 0.98     | 1.89      | 94.5%         |
| mpi   | 4                    | 0.53     | 3.49      | 87.3%         |
| mpi   | 6                    | 0.38     | 4.87      | 81.2%         |

**Анализ:**
- Приблизительно линейное ускорение до 6 процессов (4.87×).
- Эффективность остается выше 80% на 6 процессах.
- Узкие места: накладные расходы MPI_Reduce и MPI_Bcast (передача O(n) данных).
- Масштабируемость ограничена накладными расходами на коммуникацию и дисбалансом нагрузки при m ≫ size.

## 8. Выводы
1. MPI-реализация достигает значительного ускорения (4.87× на 6 процессах) при сохранении корректности.
2. Эффективность постепенно снижается из-за накладных расходов на коммуникацию.
3. Алгоритм хорошо подходит для высоких матриц (m ≫ числа процессов).
4. Ограничения: требует хранения всей матрицы в памяти; производительность ограничена количеством столбцов n (объемом передаваемых данных).

## 9. Литература

1. Документация по курсу: "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А, Нестеров А.Ю)
2. Лекции по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/