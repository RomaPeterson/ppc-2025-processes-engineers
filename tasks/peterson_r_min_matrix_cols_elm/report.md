# Поиск минимального элемента в каждом столбце матрицы

- Студент: Петерсон Роман Дмитриевич, группа 3823Б1ПР5
- Технология: MPI, SEQ
- Вариант: 18

## 1. Введение
Задача поиска минимальных элементов по столбцам матрицы является фундаментальной в обработке данных, анализе изображений и предобработке данных для машинного обучения. Для больших матриц (например, 10⁴×10⁴) последовательные вычисления становятся узким местом. Данный проект реализует и оценивает параллельное решение на основе MPI для ускорения поиска минимумов по столбцам. Особенностью реализации является процедурная генерация матрицы, что позволяет работать с матрицами произвольного размера без необходимости их хранения в памяти целиком.

## 2. Постановка задачи
Дана квадратная целочисленная матрица A размером n × n, элементы которой генерируются детерминированной функцией на основе индексов (i, j). Требуется вычислить вектор b длины n, где:
b[j] = min{A[i][j] | i = 0..n-1} для всех j = 0..n-1.

**Формат входных данных:** Целое число n — размерность квадратной матрицы.
**Формат выходных данных:** Вектор из n целых чисел (минимумы столбцов).
**Ограничения:** n > 0; для MPI: n ≤ INT_MAX (из-за параметров MPI_Reduce и MPI_Bcast).

**Функция генерации элементов:**
```cpp
InType generate(int64_t i, int64_t j) {
    uint64_t seed = (i * 100000007ULL + j * 1000000009ULL) ^ 42ULL;
    seed ^= seed >> 12;
    seed ^= seed << 25;
    seed ^= seed >> 27;
    uint64_t value = seed * 0x2545F4914F6CDD1DULL;
    return static_cast<InType>((value % 2000001) - 1000000);
}
```
Генерируемые значения находятся в диапазоне [-1000000, 1000000].

## 3. Базовый алгоритм (последовательный)
1. Для каждого столбца j = 0 до n-1:
   - Инициализировать минимум значением первого элемента столбца: min_val = generate(0, j)
   - Для каждой строки i = 1 до n-1:
     - val = generate(i, j)
     - min_val = min(min_val, val)
   - Добавить min_val в результирующий вектор
2. Вернуть результирующий вектор.

**Сложность:** O(n²) времени, O(n) дополнительной памяти для результата.

## 4. Схема распараллеливания (MPI)
**Распределение данных:** Строки делятся равномерно между процессами. Если n % size ≠ 0, первые (n % size) процессов получают на одну строку больше.

**Коммуникационная схема:**
1. Каждый процесс инициализирует локальный вектор минимумов значением INT_MAX.
2. Каждый процесс вычисляет локальные минимумы для своего диапазона строк.
3. MPI_Reduce с операцией MPI_MIN собирает глобальные минимумы на процессе 0.
4. MPI_Bcast рассылает конечный результат всем процессам.

**Роли процессов:** Все процессы вычисляют; процесс 0 выступает корневым для редукции и рассылки.

**Псевдокод:**
```
rows_per_process = n / size
leftover = n % size
process_rows = rows_per_process + (rank < leftover ? 1 : 0)
first_row = rank * rows_per_process + min(rank, leftover)
last_row = first_row + process_rows

local_min[0:n-1] = INT_MAX
для i от first_row до last_row-1:
    для j = 0 до n-1:
        val = generate(i, j)
        local_min[j] = min(local_min[j], val)

MPI_Reduce(local_min → global_min, MPI_MIN, root=0)
MPI_Bcast(global_min от root=0)
```

## 5. Детали реализации
**Структура кода:**
- `common.hpp` – Общие определения типов (InType = int, OutType = std::vector<int>, TestType, BaseTask)
- `ops_seq.hpp/cpp` – Последовательная реализация
- `ops_mpi.hpp/cpp` – MPI-параллельная реализация
- `func_tests.cpp` – Функциональные тесты
- `perf_tests.cpp` – Производительные тесты

**Ключевые классы:**
- `PetersonRMinMatrixSEQ` – Последовательная задача
- `PetersonRMinMatrixMPI` – MPI-параллельная задача
Оба наследуются от `BaseTask` (ppc::task::Task<int, std::vector<int>>).

**Методы жизненного цикла задачи:**
- `ValidationImpl()` — проверяет, что входное значение n > 0 и выходной вектор пуст
- `PreProcessingImpl()` — очищает и резервирует память для выходного вектора
- `RunImpl()` — основной алгоритм поиска минимумов
- `PostProcessingImpl()` — проверяет корректность размера результата

**Особенности реализации:**
- Матрица не хранится в памяти — элементы вычисляются "на лету" при обходе
- Детерминированная генерация обеспечивает воспроизводимость результатов
- SEQ версия обходит матрицу по столбцам для лучшей локальности кэша при построении результата
- MPI версия обходит матрицу по строкам для эффективного распределения работы

**Граничные случаи:**
- n = 0 отклоняется на этапе валидации
- n = 1 — единственный элемент является минимумом
- n, не кратное числу процессов — корректная балансировка нагрузки

**Использование памяти:** 
- SEQ: O(n) для результата
- MPI: O(n) на каждом процессе для локальных и глобальных минимумов

## 6. Экспериментальная установка
**Аппаратное обеспечение/ОС:**
- Процессор: Intel Core i5-9400F (6 ядер, 6 потоков) @ 2.90 ГГц
- ОЗУ: 16 ГБ DDR4
- ОС: Ubuntu 20.04.4 LTS

**Инструментарий:**
- Компилятор: GCC 9.4.0
- Тип сборки: RelWithDebInfo (оптимизация -O3)
- MPI: OpenMPI 4.0.3
- Тестирование: GoogleTest 1.11.0

**Окружение:**
- MPI-процессы: 1–6 (задаются через `mpirun -np N`)

**Тестовые данные:** 
- Функциональные тесты: матрицы размерами 1×1, 2×2, 3×3, 5×5, 17×17, 64×64, 99×99, 100×100, 128×128, 256×256, 512×512
- Производительный тест: матрица 10000×10000 (10⁸ элементов)

## 7. Результаты и обсуждение

### 7.1 Корректность
Проверено с помощью:
- Валидационных тестов на невалидные входные данные (n = 0)
- Функциональных параметризованных тестов с различными размерами матриц
- Тестов повторного использования задачи (проверка корректной очистки состояния)
- Перекрестной проверки выходных данных SEQ и MPI реализаций
- Сравнения с эталонными минимумами, вычисленными функцией `CalculateExpectedColumnMins()`

### 7.2 Производительность
Результаты для матрицы 10000×10000:

| Режим | Количество процессов | Время, с | Ускорение | Эффективность |
|-------|----------------------|----------|-----------|---------------|
| seq   | 1                    | 1.85     | 1.00      | –             |
| mpi   | 2                    | 0.98     | 1.89      | 94.5%         |
| mpi   | 4                    | 0.53     | 3.49      | 87.3%         |
| mpi   | 6                    | 0.38     | 4.87      | 81.2%         |

**Анализ:**
- Приблизительно линейное ускорение до 6 процессов (4.87×).
- Эффективность остается выше 80% на 6 процессах.
- Основные вычислительные затраты — генерация элементов матрицы (xorshift операции).
- Узкие места: накладные расходы MPI_Reduce и MPI_Bcast (передача O(n) данных).
- Масштабируемость ограничена накладными расходами на коммуникацию при малом отношении n/size.

## 8. Выводы
1. MPI-реализация достигает значительного ускорения (4.87× на 6 процессах) при сохранении корректности.
2. Процедурная генерация матрицы позволяет работать с матрицами любого размера без ограничений по памяти.
3. Эффективность постепенно снижается из-за накладных расходов на коммуникацию.
4. Алгоритм хорошо подходит для больших квадратных матриц (n ≫ числа процессов).
5. Детерминированная генерация обеспечивает воспроизводимость тестов и корректную верификацию результатов.
6. Ограничения: производительность ограничена количеством столбцов n (объемом передаваемых данных в MPI_Reduce/MPI_Bcast).

## 9. Литература

1. Документация по курсу: "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А, Нестеров А.Ю)
2. Лекции по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/