# Метод Гаусса – ленточная горизонтальная схема

- Студент: Петерсон Роман Дмитриевич, группа 3823Б1ПР5  
- Технология: MPI, SEQ  
- Вариант: 15  

## 1. Введение
Метод Гаусса является фундаментальным алгоритмом решения систем линейных уравнений (СЛАУ). Для ленточных матриц с ограниченной шириной ленты можно применять специализированные схемы вычислений, которые значительно повышают производительность. Данный проект реализует последовательную и MPI-параллельную версии метода Гаусса с горизонтальным распределением данных для ленточных матриц, что позволяет эффективно решать большие системы уравнений.

## 2. Постановка задачи
Решить систему n линейных уравнений с n неизвестными: Ax = b, где A — ленточная матрица с шириной ленты w (элементы aᵢⱼ = 0 при |i-j| > w).

**Формат входных данных:** Расширенная матрица размером n × (n+1) типа `std::vector<std::vector<double>>`, где первые n столбцов представляют матрицу A, а последний столбец — вектор b.  
**Формат выходных данных:** Вектор решения x длины n типа `std::vector<double>`.  
**Ограничения:** 
- Матрица должна быть непустой
- Количество столбцов должно быть не менее n+1
- Все строки должны иметь одинаковую длину
- Матрица не должна быть вырожденной (|aₖₖ| > 1e-10 после выбора ведущего элемента)

## 3. Базовый алгоритм (последовательный)
1. **Прямой ход (триангуляция):**
   - Для каждого ведущего элемента k (0 ≤ k < n):
     - Поиск ведущей строки: найти строку max_row с максимальным |aᵢₖ| для i ≥ k
     - Перестановка строк k и max_row при необходимости
     - Проверка на вырожденность: если |aₖₖ| < 1e-10, вернуть ошибку
     - Исключение: для каждой строки i > k:
       - factor = aᵢₖ / aₖₖ
       - Для j от k до cols-1: aᵢⱼ = aᵢⱼ - factor × aₖⱼ

2. **Обратный ход:**
   - Для i от n-1 до 0:
     - sum = Σⱼ₌ᵢ₊₁ⁿ⁻¹ aᵢⱼ × xⱼ
     - xᵢ = (bᵢ - sum) / aᵢᵢ

**Сложность:** O(n³) для плотных матриц, O(n×w²) для ленточных матриц с шириной ленты w.  
**Память:** O(n×(n+1)) для хранения расширенной матрицы.

## 4. Схема распараллеливания (MPI)

**Распределение данных (горизонтальная схема):** Строки распределяются циклически (round-robin) между процессами. Строка i назначается процессу (i % size).

**Коммуникационная схема:**

1. **Распределение строк (DistributeRows):**
   - Процесс 0 отправляет строки соответствующим процессам через `MPI_Send`
   - Каждый процесс получает свои строки через `MPI_Recv`
   - Создается маппинг глобальных индексов на локальные (`global_to_local`)

2. **Прямой ход (ForwardEliminationMPI):**
   - Для каждой ведущей строки k:
     - Определение владельца: owner_process = k % size
     - Владелец подготавливает ведущую строку
     - `MPI_Bcast` рассылает ведущую строку всем процессам
     - Каждый процесс исключает ведущую строку из своих локальных строк с индексами > k

3. **Обратный ход (BackSubstitutionMPI):**
   - Для i от n-1 до 0:
     - owner_process = i % size
     - Владелец вычисляет xᵢ, используя уже известные xⱼ (j > i)
     - `MPI_Bcast` рассылает xᵢ всем процессам

**Псевдокод (MPI):**
```
// Распределение строк
для i = 0 до n-1:
    владелец = i % size
    если rank == 0 и владелец != 0:
        MPI_Send(строка[i], владельцу)
    если rank == владелец и rank != 0:
        MPI_Recv(локальная_строка)

// Прямой ход
для k = 0 до n-1:
    владелец = k % size
    если rank == владелец:
        pivot_row = локальная_строка[k]
    MPI_Bcast(pivot_row, владелец)
    
    если |pivot_row[k]| < 1e-10:
        return false
    
    для каждой локальной строки i с глобальным индексом > k:
        если |строка[k]| > 1e-10:
            factor = строка[k] / pivot_row[k]
            строка -= factor * pivot_row

// Обратный ход
x[0..n-1] = 0
для i = n-1 до 0:
    владелец = i % size
    если rank == владелец:
        sum = Σ(строка[j] * x[j]) для j > i
        x[i] = (строка[cols-1] - sum) / строка[i]
    MPI_Bcast(x[i], владелец)
```

## 5. Детали реализации

**Структура кода:**
- `common.hpp` – Определения типов (InType, OutType, TestType, BaseTask)
- `ops_seq.hpp/cpp` – Последовательный метод Гаусса с частичным выбором ведущего элемента
- `ops_mpi.hpp/cpp` – MPI-параллельная версия с горизонтальным распределением строк
- `func_tests.cpp` – Функциональные тесты
- `perf_tests.cpp` – Производительные тесты

**Ключевые классы:**

`PetersonRGaussBandHorizontalSchemeSEQ`:
- Статические методы: `ForwardElimination`, `FindPivotRow`, `EliminateColumn`, `BackSubstitution`
- Использует частичный выбор ведущего элемента для численной устойчивости

`PetersonRGaussBandHorizontalSchemeMPI`:
- Статические методы: `ValidateInput`, `DistributeRows`, `ForwardEliminationMPI`, `EliminateColumnMPI`, `GetGlobalIndex`, `BackSubstitutionMPI`
- Циклическое распределение строк для балансировки нагрузки
- Синхронизация через `MPI_Bcast` на каждом шаге

**Методы жизненного цикла задачи:**
- `ValidationImpl()` — проверяет корректность размеров матрицы (непустая, n+1 столбцов, одинаковая длина строк)
- `PreProcessingImpl()` — инициализирует пустой выходной вектор
- `RunImpl()` — выполняет прямой и обратный ход метода Гаусса
- `PostProcessingImpl()` — проверяет, что результат непустой

**Особенности реализации:**
- MPI-версия: валидация выполняется только на процессе 0, результат рассылается через `MPI_Bcast`
- Конструктор MPI-версии загружает данные только на процессе 0
- Порог проверки на вырожденность: 1e-10
- Маппинг `global_to_local` позволяет восстановить глобальный индекс строки из локального

**Использование памяти:**
- SEQ: O(n²) для копии расширенной матрицы + O(n) для результата
- MPI: O(n²/size) на каждом процессе для локальных строк + O(n) для ведущей строки и результата

## 6. Экспериментальная установка

**Аппаратное обеспечение:**
- Процессор: Intel Core i5-9400F (6 ядер, 6 потоков) @ 2.90 ГГц
- ОЗУ: 16 ГБ DDR4
- ОС: Ubuntu 20.04.4 LTS

**Программное обеспечение:**
- Компилятор: GCC 9.4.0
- Тип сборки: RelWithDebInfo (оптимизация -O3)
- MPI: OpenMPI 4.0.3
- Тестирование: GoogleTest 1.11.0

**Параметры запуска:**
- MPI-процессы: 1–6 (через `mpirun -np N`)

**Тестовые данные:**

*Функциональные тесты:*
| Размер матрицы | Ширина ленты |
|----------------|--------------|
| 3×3            | 1            |
| 4×4            | 1            |
| 5×5            | 2            |
| 6×6            | 2            |
| 8×8            | 3            |
| 10×10          | 3            |

Матрицы генерируются с известным решением x = [1, 2, 3, ..., n] и диагональным преобладанием для устойчивости.

*Производительные тесты:*
- Размер матрицы: 1000×1000
- Ширина ленты: 5
- Элементы: случайные значения из [0.1, 10.0] с диагональным преобладанием

**Генерация тестовых матриц:**
```cpp
// Функциональные тесты: построение матрицы с известным решением
for (int i = 0; i < n; ++i) {
    expected_output[i] = i + 1;  // x = [1, 2, 3, ...]
    for (int j = 0; j < n; ++j) {
        if (abs(i - j) <= band_width) {
            value = (i == j) ? (n + 1) : 1.0 / (abs(i - j) + 1.0);
            A[i][j] = value;
        }
    }
    // Обеспечение диагонального преобладания
    if (A[i][i] < sum_off_diagonal)
        A[i][i] = sum_off_diagonal + 1.0;
    // b = A * x
    b[i] = sum(A[i][j] * x[j])
}
```

## 7. Результаты и обсуждение

### 7.1 Корректность

Проверка корректности выполнена:
- **Валидационные тесты:** проверка отклонения пустых матриц, матриц с неправильным числом столбцов, матриц с разной длиной строк
- **Функциональные параметризованные тесты:** 6 конфигураций (размер × ширина ленты) для SEQ и MPI версий
- **Проверка решения:** сравнение с эталонным решением x = [1, 2, ..., n] с точностью 1e-5
- **Перекрестная проверка:** результаты SEQ и MPI версий совпадают

### 7.2 Производительность

Результаты для ленточной матрицы 1000×1000 (ширина ленты = 5):

| Режим | Процессы | Время, с | Ускорение | Эффективность |
|-------|----------|----------|-----------|---------------|
| SEQ   | 1        | 2.14     | 1.00      | –             |
| MPI   | 2        | 1.18     | 1.81      | 90.5%         |
| MPI   | 4        | 0.67     | 3.19      | 79.8%         |
| MPI   | 6        | 0.49     | 4.37      | 72.8%         |

**Анализ:**
- Достигнуто ускорение 4.37× на 6 процессах
- Эффективность снижается с увеличением числа процессов из-за:
  - **Коммуникационных накладных расходов:** n вызовов `MPI_Bcast` в прямом ходе + n вызовов в обратном ходе
  - **Синхронизации:** каждый процесс ждет ведущую строку перед исключением
  - **Дисбаланса нагрузки:** при циклическом распределении разные процессы могут иметь разное количество ненулевых элементов
- Ленточная структура уменьшает объем вычислений на строку, что увеличивает относительную долю коммуникаций

**Ограничения масштабируемости:**
- 2n операций `MPI_Bcast` (n в прямом ходе, n в обратном)
- Последовательная зависимость: строка k+1 не может быть обработана до получения ведущей строки k
- Для узких лент (малое w) вычислительная нагрузка мала, коммуникации доминируют

## 8. Выводы

1. **Корректность:** Реализованный метод Гаусса с горизонтальным распределением строк корректно решает СЛАУ с ленточными матрицами. Результаты SEQ и MPI версий совпадают с точностью 1e-5.

2. **Производительность:** Достигнуто ускорение 4.37× на 6 процессах при эффективности 72.8%.

3. **Схема распределения:** Циклическое (round-robin) распределение строк обеспечивает равномерную балансировку нагрузки для матриц с регулярной структурой.

4. **Численная устойчивость:** Частичный выбор ведущего элемента в SEQ версии повышает устойчивость. В MPI версии используется фиксированный порядок строк.

5. **Ограничения:**
   - Плотное хранение неэффективно для очень разреженных ленточных матриц
   - Большое количество синхронизаций ограничивает масштабируемость
   - MPI версия не выполняет перестановку строк (предполагается диагональное преобладание)

**Возможные улучшения:**
- Реализация разреженного хранения для экономии памяти
- Конвейеризация: начало исключения до завершения рассылки
- Блочная версия алгоритма для уменьшения числа коммуникаций
- Добавление выбора ведущего элемента в MPI версию

## 9. Литература

1. Документация по курсу: "Параллельное программирование": https://learning-process.github.io/parallel_programming_course/ru/index.html (Оболенский А.А, Нестеров А.Ю)
2. Лекции по курсу "Параллельное программирование". (Сысоев А.В. ННГУ 2025 г.)
3. Документация по MPI: https://www.open-mpi.org/